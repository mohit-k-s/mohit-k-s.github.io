<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Mohit&#39;s Page</title>
    <link>https://mohit-k-s.github.io/blog/</link>
    <description>Recent content in Blog on Mohit&#39;s Page</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <managingEditor>mohit.bash@gmail.com (Mohit Kumar Singh)</managingEditor>
    <webMaster>mohit.bash@gmail.com (Mohit Kumar Singh)</webMaster>
    <copyright>Mohit Kumar Singh</copyright>
    <lastBuildDate>Sun, 27 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mohit-k-s.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From Static Analysis to AI: My Journey Building a Git Diff Impact Analyzer</title>
      <link>https://mohit-k-s.github.io/blog/git_impact/</link>
      <pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate><author>mohit.bash@gmail.com (Mohit Kumar Singh)</author>
      <guid>https://mohit-k-s.github.io/blog/git_impact/</guid>
      <description>&lt;h2 id=&#34;you-changed-a-line-what-did-you-break&#34;&gt;You Changed a Line. What Did You Break?&lt;/h2&gt;&#xA;&lt;p&gt;You push a change. CI passes. Days later, production breaks. Why?&lt;br&gt;&#xA;Because that &amp;ldquo;simple&amp;rdquo; change rippled into places you didn&amp;rsquo;t know existed.&lt;/p&gt;&#xA;&lt;p&gt;I got tired of tools telling me &lt;em&gt;what changed&lt;/em&gt;, but never &lt;em&gt;what it meant&lt;/em&gt;.&lt;br&gt;&#xA;So I built a system that answers the real question:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;&amp;ldquo;What changed‚Äîand why does it matter?&amp;rdquo;&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;This post is about how I moved beyond syntax-based static analysis and into &lt;strong&gt;semantic impact analysis using LLMs&lt;/strong&gt;.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="you-changed-a-line-what-did-you-break">You Changed a Line. What Did You Break?</h2>
<p>You push a change. CI passes. Days later, production breaks. Why?<br>
Because that &ldquo;simple&rdquo; change rippled into places you didn&rsquo;t know existed.</p>
<p>I got tired of tools telling me <em>what changed</em>, but never <em>what it meant</em>.<br>
So I built a system that answers the real question:</p>
<blockquote>
<p><strong>&ldquo;What changed‚Äîand why does it matter?&rdquo;</strong></p></blockquote>
<p>This post is about how I moved beyond syntax-based static analysis and into <strong>semantic impact analysis using LLMs</strong>.</p>
<hr>
<h1 id="part-1-the-traditional-approach">Part 1: The Traditional Approach</h1>
<h2 id="my-first-attempt-static-analysis">My First Attempt: Static Analysis</h2>
<p>I started with what most engineers try first: writing my own static analyzer. I called it <code>reverse-mapper</code>. The idea was to trace every file, function, and variable, and then reverse-map how a change propagates.</p>
<h3 id="the-3-phase-pipeline">The 3-Phase Pipeline</h3>
<ol>
<li><strong>Dependency Tree</strong>: Entry point detection and BFS traversal (<code>index.js</code>, <code>app.js</code>, etc.)</li>
<li><strong>Function &amp; Variable Mapping</strong>: Track usage, definitions, and references</li>
<li><strong>Reverse Impact Mapping</strong>: Link every variable to the functions and APIs it might affect</li>
</ol>
<p>It worked. Kind of.</p>
<h2 id="the-reality-check-why-static-analysis-wasnt-enough">The Reality Check: Why Static Analysis Wasn&rsquo;t Enough</h2>
<p>Once I started using it on real projects, the limitations became obvious:</p>
<ul>
<li>üß© <strong>No semantic understanding</strong>: It couldn&rsquo;t infer business logic</li>
<li>üî® <strong>Maintenance nightmare</strong>: JS, TS, different module systems, ASTs</li>
<li>üßµ <strong>No runtime or conditional logic</strong>: Missed dynamic behavior</li>
<li>üß† <strong>No ‚Äúwhy‚Äù behind the changes</strong></li>
</ul>
<hr>
<h1 id="part-2-the-ai-revolution">Part 2: The AI Revolution</h1>
<h2 id="the-breakthrough-semantic-understanding-with-llms">The Breakthrough: Semantic Understanding with LLMs</h2>
<p>What if instead of just parsing code, I asked an LLM to <strong>understand</strong> it?</p>
<p>Modern LLMs like GPT-4, Claude, and Gemini can:</p>
<ul>
<li>Understand intent and business logic</li>
<li>Trace dependencies <strong>semantically</strong></li>
<li>Recommend what tests to run</li>
<li>Evaluate <strong>risk</strong> and <strong>impact</strong></li>
<li>Suggest <strong>deployment strategies</strong></li>
</ul>
<p>This was a shift from &ldquo;what is connected?&rdquo; to <strong>&ldquo;why does this change matter?&rdquo;</strong></p>
<h2 id="building-the-ai-powered-analyzer">Building the AI-Powered Analyzer</h2>
<p>I scrapped the AST parsing and built a much simpler‚Äîbut more powerful‚Äîpipeline:</p>
<h3 id="the-4-step-ai-pipeline">The 4-Step AI Pipeline</h3>
<ol>
<li><strong>üîç Extract the Git Diff</strong></li>
<li><strong>üìÅ Gather Relevant Files</strong> (imports, touched files, neighbors)</li>
<li><strong>üß† Ask the AI</strong> (via structured prompts)</li>
<li><strong>üìä Return Actionable Insights</strong></li>
</ol>
<h3 id="sample-analysis-output">Sample Analysis Output</h3>





<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">  <span class="nt">&#34;impactedFiles&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="s2">&#34;handlers/user-service.js&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="s2">&#34;frontend/components/UserList.js&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    <span class="s2">&#34;lib/cache.js&#34;</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">  <span class="p">],</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">  <span class="nt">&#34;riskLevel&#34;</span><span class="p">:</span> <span class="s2">&#34;medium&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">  <span class="nt">&#34;testingRecommendations&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">    <span class="s2">&#34;Test user retrieval endpoints&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">    <span class="s2">&#34;Verify inactive users are filtered correctly&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    <span class="s2">&#34;Check cache invalidation behavior&#34;</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">  <span class="p">],</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl">  <span class="nt">&#34;deploymentNotes&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">    <span class="s2">&#34;Monitor user API response times&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">    <span class="s2">&#34;Watch for database query performance issues&#34;</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl">  <span class="p">],</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl">  <span class="nt">&#34;confidence&#34;</span><span class="p">:</span> <span class="mf">0.87</span><span class="p">,</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl">  <span class="nt">&#34;explanation&#34;</span><span class="p">:</span> <span class="s2">&#34;Adding WHERE active = 1 filters inactive users, which affects the user service and downstream components that display user lists...&#34;</span>
</span></span><span class="line"><span class="ln">19</span><span class="cl"><span class="p">}</span></span></span></code></pre></div><h2 id="the-challenge-scaling-ai-analysis">The Challenge: Scaling AI Analysis</h2>
<p>The obvious limitation with AI analysis is context windows. Large codebases can&rsquo;t fit in a single prompt, and even if they could, the analysis quality degrades significantly with massive context.</p>
<p>My solution was <strong>intelligent batching</strong>:</p>
<ul>
<li><strong>Token-aware splitting</strong> - Estimate token count and create optimal batches</li>
<li><strong>Rate limiting</strong> - Respect API limits with delays between requests</li>
<li><strong>Exponential backoff</strong> - Retry failed requests with increasing delays</li>
<li><strong>Graceful degradation</strong> - Continue analysis even if some batches fail</li>
<li><strong>Smart merging</strong> - Combine batch results into a unified analysis</li>
</ul>
<p>This approach lets me analyze projects of any size while maintaining quality and staying within API limits.</p>
<h2 id="multiple-ai-providers-options-for-every-need">Multiple AI Providers: Options for Every Need</h2>
<p>I didn&rsquo;t want to lock myself into a single AI provider, so I built support for multiple options:</p>
<h3 id="cloud-models">Cloud Models</h3>
<ul>
<li><strong>Google Gemini</strong> - Best value for code analysis with huge context windows</li>
<li><strong>OpenAI GPT-4</strong> - Highest quality but most expensive</li>
<li><strong>Anthropic Claude</strong> - Good balance of quality and cost</li>
</ul>
<h3 id="local-models-ollama">Local Models (Ollama)</h3>
<ul>
<li><strong>CodeLlama</strong> - Specialized for programming tasks</li>
<li><strong>Mistral</strong> - Fast general-purpose analysis</li>
<li><strong>DeepSeek Coder</strong> - Optimized for speed on smaller systems</li>
</ul>
<p>The local model integration was crucial for privacy-conscious projects where sending code to external APIs wasn&rsquo;t acceptable.</p>
<h2 id="local-vs-cloud-the-trade-offs-i-discovered">Local vs Cloud: The Trade-offs I Discovered</h2>
<p>Through extensive testing, I found clear patterns in when to use each approach:</p>
<table>
  <thead>
      <tr>
          <th>Factor</th>
          <th>Local Models (Ollama)</th>
          <th>Cloud Models</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Privacy</strong></td>
          <td>Perfect - code stays local</td>
          <td>Risky - code sent externally</td>
      </tr>
      <tr>
          <td><strong>Cost</strong></td>
          <td>Free after setup</td>
          <td>$0.50-2.00 per analysis</td>
      </tr>
      <tr>
          <td><strong>Speed</strong></td>
          <td>Slower (45-90 seconds)</td>
          <td>Faster (15-30 seconds)</td>
      </tr>
      <tr>
          <td><strong>Quality</strong></td>
          <td>Good (75% accuracy)</td>
          <td>Excellent (85% accuracy)</td>
      </tr>
      <tr>
          <td><strong>Setup</strong></td>
          <td>Complex - requires local resources</td>
          <td>Simple - just API key</td>
      </tr>
  </tbody>
</table>
<p>For my workflow, I use local models for regular development and reserve cloud models for critical production analysis.</p>
<h2 id="performance-benchmarks-the-reality">Performance Benchmarks: The Reality</h2>
<p>I benchmarked both approaches on a real 50,000-line codebase:</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Analysis Time</th>
          <th>Accuracy</th>
          <th>Cost</th>
          <th>Best Use Case</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Traditional Static</strong></td>
          <td>2-3 seconds</td>
          <td>60%</td>
          <td>Free</td>
          <td>CI/CD quick checks</td>
      </tr>
      <tr>
          <td><strong>AI Cloud</strong></td>
          <td>15-30 seconds</td>
          <td>85%</td>
          <td>$0.50-2.00</td>
          <td>Comprehensive reviews</td>
      </tr>
      <tr>
          <td><strong>AI Local</strong></td>
          <td>45-90 seconds</td>
          <td>75%</td>
          <td>Free</td>
          <td>Privacy-focused analysis</td>
      </tr>
  </tbody>
</table>
<h2 id="real-world-impact-before-vs-after">Real-World Impact: Before vs After</h2>
<h3 id="before-traditional-static-analysis">Before (Traditional Static Analysis)</h3>





<pre tabindex="0"><code>‚ùå Found 23 potentially impacted files
‚ùå 156 function dependencies detected  
‚ùå No context about WHY changes matter
‚ùå High false positive rate
‚ùå No actionable recommendations
‚ùå Developers ignored the output</code></pre><h3 id="after-ai-powered-analysis">After (AI-Powered Analysis)</h3>





<pre tabindex="0"><code>‚úÖ Identified 8 actually impacted files
‚úÖ Clear risk assessment: &#34;Medium risk - affects user auth flow&#34;
‚úÖ Specific testing: &#34;Test login with expired tokens&#34;
‚úÖ Deployment guidance: &#34;Deploy during low-traffic hours&#34;
‚úÖ Business context: &#34;Improves security but may cause temporary logouts&#34;
‚úÖ Developers actually use and trust the analysis</code></pre><h2 id="lessons-learned-when-ai-wins-and-when-it-doesnt">Lessons Learned: When AI Wins and When It Doesn&rsquo;t</h2>
<p>After months of using both approaches, here are my key insights:</p>
<h3 id="ai-is-superior-for">AI is Superior For:</h3>
<ul>
<li><strong>Understanding business impact</strong> of code changes</li>
<li><strong>Providing actionable recommendations</strong> for testing and deployment</li>
<li><strong>Explaining the &ldquo;why&rdquo;</strong> behind impacts, not just the &ldquo;what&rdquo;</li>
<li><strong>Handling complex, indirect relationships</strong> that static analysis misses</li>
<li><strong>Cross-language analysis</strong> without building new parsers</li>
</ul>
<h3 id="traditional-static-analysis-still-wins-for">Traditional Static Analysis Still Wins For:</h3>
<ul>
<li><strong>Speed</strong> - When you need instant feedback</li>
<li><strong>Deterministic results</strong> - Same input always gives same output</li>
<li><strong>No external dependencies</strong> - Works offline, no API costs</li>
<li><strong>CI/CD pipelines</strong> - Fast enough for every commit</li>
</ul>
<h3 id="the-hybrid-approach">The Hybrid Approach</h3>
<p>My current setup uses both:</p>
<ul>
<li>Traditional analysis for pre-commit hooks (speed)</li>
<li>AI analysis for pull request reviews (comprehensive)</li>
<li>Local AI models for privacy-sensitive projects</li>
<li>Cloud AI models for critical production changes</li>
</ul>
<h2 id="the-technical-architecture">The Technical Architecture</h2>
<p>The final system has these key components:</p>





<pre tabindex="0"><code>Git Diff ‚Üí Context Gathering ‚Üí Intelligent Batching ‚Üí AI Analysis ‚Üí Result Merging
    ‚Üì              ‚Üì                    ‚Üì               ‚Üì            ‚Üì
Extract       Find Related        Create Optimal    Multiple     Unified
Changes       Files/Imports       Token Batches     LLM Calls    Report</code></pre><p>The beauty is in its simplicity compared to the traditional approach which required:</p>
<ul>
<li>Entry point detection</li>
<li>BFS dependency traversal</li>
<li>AST parsing for multiple languages</li>
<li>Complex variable usage analysis</li>
<li>Manual rule definition</li>
</ul>
<h2 id="future-possibilities">Future Possibilities</h2>
<p>This AI-powered approach opens up exciting possibilities I never considered with static analysis:</p>
<ul>
<li><strong>Automated code reviews</strong> with semantic understanding</li>
<li><strong>Risk-based testing</strong> - AI determines which tests to prioritize</li>
<li><strong>Intelligent deployment strategies</strong> - AI guides rollout based on change impact</li>
<li><strong>Technical debt analysis</strong> - Understanding code quality implications</li>
<li><strong>Cross-team notifications</strong> - AI identifies which teams need to know about changes</li>
</ul>
<h2 id="the-bottom-line">The Bottom Line</h2>
<p>Static analysis served its purpose, but AI represents a fundamental paradigm shift in how I understand code changes. By leveraging the semantic understanding of large language models, I can:</p>
<ul>
<li><strong>Understand code contextually</strong>, not just syntactically</li>
<li><strong>Get actionable insights</strong> instead of raw connection data</li>
<li><strong>Scale to any codebase</strong> with intelligent batching</li>
<li><strong>Choose the right tool</strong> for each situation (traditional, cloud AI, local AI)</li>
<li><strong>Actually trust and use</strong> the analysis results</li>
</ul>
<p>The era of AI-powered development tools is here. Traditional static analysis isn&rsquo;t dead‚Äîit still has its place for speed-critical use cases. But for understanding the true impact of code changes, AI is simply superior.</p>
<p>If you&rsquo;re still relying on manual code review and intuition to understand change impact, you&rsquo;re missing out on a transformative approach that can make your development process safer, faster, and more intelligent.</p>
<h2 id="whats-next">What&rsquo;s Next?</h2>
<p>I&rsquo;m continuing to refine this approach with:</p>
<ul>
<li>Better prompt engineering for more accurate analysis</li>
<li>Custom fine-tuning for specific codebases and domains</li>
<li>Integration with more development tools and workflows</li>
<li>Hybrid models that combine the speed of static analysis with the intelligence of AI</li>
</ul>
<p>The future of code analysis is here, and it&rsquo;s powered by artificial intelligence.</p>
<hr>
<p><em>Check out the <a href="https://github.com/mohit-k-s/git-diff-impact-analyzer">project repository</a> and start understanding your code changes like never before. Ofcourse like everything this needs to be improved too</em></p>
]]></content:encoded>
    </item>
    <item>
      <title>Creativity</title>
      <link>https://mohit-k-s.github.io/blog/creativity/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate><author>mohit.bash@gmail.com (Mohit Kumar Singh)</author>
      <guid>https://mohit-k-s.github.io/blog/creativity/</guid>
      <description>&lt;p&gt;Yesterday, I made my first post‚Äîand I promised myself I‚Äôd continue writing, just like I used to back in 10th grade. There‚Äôs something deeply fulfilling about writing. I‚Äôve missed that feeling for a long time, and now I‚Äôm trying to find my way back to it. So here I am. We‚Äôd start with what starts things.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Creativity (noun): the use of imagination or original ideas to create something&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;Creativity, as we all know, is one of the fundamental pillars on which the entire arc of human evolution rests.&#xA;If not for original ideas, we‚Äôd still be hunting for food and hiding from the cold.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p>Yesterday, I made my first post‚Äîand I promised myself I‚Äôd continue writing, just like I used to back in 10th grade. There‚Äôs something deeply fulfilling about writing. I‚Äôve missed that feeling for a long time, and now I‚Äôm trying to find my way back to it. So here I am. We‚Äôd start with what starts things.</p>
<blockquote>
<p>Creativity (noun): the use of imagination or original ideas to create something</p></blockquote>
<p>Creativity, as we all know, is one of the fundamental pillars on which the entire arc of human evolution rests.
If not for original ideas, we‚Äôd still be hunting for food and hiding from the cold.</p>
<p>At its core, creativity is the act of combining our mental capacities and taking what‚Äôs already available and reshaping it into something new.
This new ‚Äúthing‚Äù doesn‚Äôt have to be grand. It can be a poem, a cooling machine, a ziplock bag, or a story whispered by firelight.</p>
<p>Creativity is not limited to the arts. It lives in engineering, cooking, survival, protest, play. It‚Äôs how we‚Äôve adapted, improved, and often just endured.
It‚Äôs invention born not only from inspiration, but from constraint.</p>
<p>When was the last time you created something ? When was the last time you sat down and just gave your best to do something by yourself , with just what you know. There‚Äôs something about that feeling, right?</p>
<p>That quiet intensity. That sense of this is my work.</p>
<p>Of course, people take help‚Äîand that‚Äôs not wrong. We‚Äôre not meant to be experts at everything.
But still‚Äîthere‚Äôs a certain joy, a deep satisfaction, in being completely immersed in the one thing you love.
To create something original.</p>
<p>I enjoy writing and you might enjoy knitting, that last piece in which you made a cool pattern surely is the word of mouth.</p>
<p>Why ?
Creating is often linked to immediate benefits‚Äîfor society, and for yourself.
Make something useful, and the world might reward you with recognition, money, maybe even status.</p>
<p>That‚Äôs fine. There‚Äôs no shame in it. But pause for a moment.
Ask yourself: Do you create to live, or live to create?</p>
<p>There‚Äôs something quietly romantic‚Äîalmost defiant‚Äîabout calling yourself an original.
About making something not because it‚Äôs useful, but because it had to be made.
Not for profit, not for praise. Just for the act. For the joy.</p>
<p>No judgment, though. I respect all creators‚Äîthe practical, the poetic, the ones who ship, and the ones who scribble on napkins and never show a soul.</p>
<p>But if you‚Äôre someone who feels that strange pull to make things that don‚Äôt quite have a reason yet, keep going.
That‚Äôs the kind of creation that changes things.</p>
<p>How ?
The why is simple‚Äîit‚Äôs the spark that gets you going. But it‚Äôs not enough. To actually create something, you have to put in the hours. The people you admire weren‚Äôt creators from the start‚Äîthey were learners. That‚Äôs the part people often skip. You have to be willing to learn, to fail, and to repeat the process over and over again. Obsess over it.</p>
<p>Create with so much intensity that it borders on madness‚Äîbecause the truth is, creativity isn‚Äôt a lightning strike.</p>
<p>In this era of knowledge, everything is within reach. Imagine Newton‚Äîthe greatest of all time‚Äîhaving access to the tools and information we have today. The possibilities would‚Äôve been endless. But here‚Äôs the thing: you also live in this era. You, too, can do what you dream of. It‚Äôs just not going to happen overnight. Real creation takes time.</p>
<p>And it will demand things from you that most people aren‚Äôt willing to give up‚Äîcomfort, distraction, certainty. I know that‚Äôs debatable, but it‚Äôs often the trade for creating something truly great. History is full of that proof. The people who changed things didn‚Äôt always live balanced lives‚Äîbut they lived driven ones. If you‚Äôre willing to pay that price, even in small doses, the doors are wide open.</p>
<p>But does that mean you can‚Äôt call small wins a win? Not at all. That‚Äôs the best part. In the art of creation, everything new is new. Every small step forward counts. A sentence written, a sketch finished, a bug finally fixed‚Äîthey all matter. You don‚Äôt need to change the world to call yourself a creator. You just need to keep creating.</p>
<p>What ?
This one‚Äôs simple‚Äîit can be anything. Anything that already exists and just needs a little polishing, or something that doesn‚Äôt exist yet at all. It could be a new music piece, a sketch, a blog post, an app , or even your next dish.</p>
<p>The ‚Äúwhat‚Äù of creation isn‚Äôt fixed. That‚Äôs the beauty of it. There are a thousand answers, and they‚Äôre all valid. What matters is that you decide to make something.
</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
